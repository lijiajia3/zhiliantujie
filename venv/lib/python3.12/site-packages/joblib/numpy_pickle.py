





import pickle
import os
import warnings
import io
from pathlib import Path

from .compressor import lz4, LZ4_NOT_INSTALLED_ERROR
from .compressor import _COMPRESSORS, register_compressor, BinaryZlibFile
from .compressor import (ZlibCompressorWrapper, GzipCompressorWrapper,
                         BZ2CompressorWrapper, LZMACompressorWrapper,
                         XZCompressorWrapper, LZ4CompressorWrapper)
from .numpy_pickle_utils import Unpickler, Pickler
from .numpy_pickle_utils import _read_fileobject, _write_fileobject
from .numpy_pickle_utils import _read_bytes, BUFFER_SIZE
from .numpy_pickle_utils import _ensure_native_byte_order
from .numpy_pickle_compat import load_compatibility
from .numpy_pickle_compat import NDArrayWrapper




from .numpy_pickle_compat import ZNDArrayWrapper  
from .backports import make_memmap


register_compressor('zlib', ZlibCompressorWrapper())
register_compressor('gzip', GzipCompressorWrapper())
register_compressor('bz2', BZ2CompressorWrapper())
register_compressor('lzma', LZMACompressorWrapper())
register_compressor('xz', XZCompressorWrapper())
register_compressor('lz4', LZ4CompressorWrapper())








NUMPY_ARRAY_ALIGNMENT_BYTES = 16


class NumpyArrayWrapper(object):
    

    def __init__(self, subclass, shape, order, dtype, allow_mmap=False,
                 numpy_array_alignment_bytes=NUMPY_ARRAY_ALIGNMENT_BYTES):
        
        self.subclass = subclass
        self.shape = shape
        self.order = order
        self.dtype = dtype
        self.allow_mmap = allow_mmap
        
        
        
        self.numpy_array_alignment_bytes = numpy_array_alignment_bytes

    def safe_get_numpy_array_alignment_bytes(self):
        
        
        return getattr(self, 'numpy_array_alignment_bytes', None)

    def write_array(self, array, pickler):
        
        
        buffersize = max(16 * 1024 ** 2 // array.itemsize, 1)
        if array.dtype.hasobject:
            
            
            
            pickle.dump(array, pickler.file_handle, protocol=2)
        else:
            numpy_array_alignment_bytes = \
                self.safe_get_numpy_array_alignment_bytes()
            if numpy_array_alignment_bytes is not None:
                current_pos = pickler.file_handle.tell()
                pos_after_padding_byte = current_pos + 1
                padding_length = numpy_array_alignment_bytes - (
                    pos_after_padding_byte % numpy_array_alignment_bytes)
                
                
                padding_length_byte = int.to_bytes(
                    padding_length, length=1, byteorder='little')
                pickler.file_handle.write(padding_length_byte)

                if padding_length != 0:
                    padding = b'\xff' * padding_length
                    pickler.file_handle.write(padding)

            for chunk in pickler.np.nditer(array,
                                           flags=['external_loop',
                                                  'buffered',
                                                  'zerosize_ok'],
                                           buffersize=buffersize,
                                           order=self.order):
                pickler.file_handle.write(chunk.tobytes('C'))

    def read_array(self, unpickler):
        
        if len(self.shape) == 0:
            count = 1
        else:
            
            
            shape_int64 = [unpickler.np.int64(x) for x in self.shape]
            count = unpickler.np.multiply.reduce(shape_int64)
        
        if self.dtype.hasobject:
            
            array = pickle.load(unpickler.file_handle)
        else:
            numpy_array_alignment_bytes = \
                self.safe_get_numpy_array_alignment_bytes()
            if numpy_array_alignment_bytes is not None:
                padding_byte = unpickler.file_handle.read(1)
                padding_length = int.from_bytes(
                    padding_byte, byteorder='little')
                if padding_length != 0:
                    unpickler.file_handle.read(padding_length)

            
            
            
            
            
            
            
            max_read_count = BUFFER_SIZE // min(BUFFER_SIZE,
                                                self.dtype.itemsize)

            array = unpickler.np.empty(count, dtype=self.dtype)
            for i in range(0, count, max_read_count):
                read_count = min(max_read_count, count - i)
                read_size = int(read_count * self.dtype.itemsize)
                data = _read_bytes(unpickler.file_handle,
                                   read_size, "array data")
                array[i:i + read_count] = \
                    unpickler.np.frombuffer(data, dtype=self.dtype,
                                            count=read_count)
                del data

            if self.order == 'F':
                array.shape = self.shape[::-1]
                array = array.transpose()
            else:
                array.shape = self.shape

        
        return _ensure_native_byte_order(array)

    def read_mmap(self, unpickler):
        
        current_pos = unpickler.file_handle.tell()
        offset = current_pos
        numpy_array_alignment_bytes = \
            self.safe_get_numpy_array_alignment_bytes()

        if numpy_array_alignment_bytes is not None:
            padding_byte = unpickler.file_handle.read(1)
            padding_length = int.from_bytes(padding_byte, byteorder='little')
            
            offset += padding_length + 1

        if unpickler.mmap_mode == 'w+':
            unpickler.mmap_mode = 'r+'

        marray = make_memmap(unpickler.filename,
                             dtype=self.dtype,
                             shape=self.shape,
                             order=self.order,
                             mode=unpickler.mmap_mode,
                             offset=offset)
        
        unpickler.file_handle.seek(offset + marray.nbytes)

        if (numpy_array_alignment_bytes is None and
                current_pos % NUMPY_ARRAY_ALIGNMENT_BYTES != 0):
            message = (
                f'The memmapped array {marray} loaded from the file '
                f'{unpickler.file_handle.name} is not byte aligned. '
                'This may cause segmentation faults if this memmapped array '
                'is used in some libraries like BLAS or PyTorch. '
                'To get rid of this warning, regenerate your pickle file '
                'with joblib >= 1.2.0. '
                'See https://github.com/joblib/joblib/issues/563 '
                'for more details'
            )
            warnings.warn(message)

        return _ensure_native_byte_order(marray)

    def read(self, unpickler):
        
        
        if unpickler.mmap_mode is not None and self.allow_mmap:
            array = self.read_mmap(unpickler)
        else:
            array = self.read_array(unpickler)

        
        if (hasattr(array, '__array_prepare__') and
            self.subclass not in (unpickler.np.ndarray,
                                  unpickler.np.memmap)):
            
            new_array = unpickler.np.core.multiarray._reconstruct(
                self.subclass, (0,), 'b')
            return new_array.__array_prepare__(array)
        else:
            return array





class NumpyPickler(Pickler):
    

    dispatch = Pickler.dispatch.copy()

    def __init__(self, fp, protocol=None):
        self.file_handle = fp
        self.buffered = isinstance(self.file_handle, BinaryZlibFile)

        
        
        if protocol is None:
            protocol = pickle.DEFAULT_PROTOCOL

        Pickler.__init__(self, self.file_handle, protocol=protocol)
        
        try:
            import numpy as np
        except ImportError:
            np = None
        self.np = np

    def _create_array_wrapper(self, array):
        
        order = 'F' if (array.flags.f_contiguous and
                        not array.flags.c_contiguous) else 'C'
        allow_mmap = not self.buffered and not array.dtype.hasobject

        kwargs = {}
        try:
            self.file_handle.tell()
        except io.UnsupportedOperation:
            kwargs = {'numpy_array_alignment_bytes': None}

        wrapper = NumpyArrayWrapper(type(array),
                                    array.shape, order, array.dtype,
                                    allow_mmap=allow_mmap,
                                    **kwargs)

        return wrapper

    def save(self, obj):
        
        if self.np is not None and type(obj) in (self.np.ndarray,
                                                 self.np.matrix,
                                                 self.np.memmap):
            if type(obj) is self.np.memmap:
                
                obj = self.np.asanyarray(obj)

            
            wrapper = self._create_array_wrapper(obj)
            Pickler.save(self, wrapper)

            
            
            
            
            
            if self.proto >= 4:
                self.framer.commit_frame(force=True)

            
            wrapper.write_array(obj, self)
            return

        return Pickler.save(self, obj)


class NumpyUnpickler(Unpickler):
    

    dispatch = Unpickler.dispatch.copy()

    def __init__(self, filename, file_handle, mmap_mode=None):
        
        
        self._dirname = os.path.dirname(filename)

        self.mmap_mode = mmap_mode
        self.file_handle = file_handle
        
        self.filename = filename
        self.compat_mode = False
        Unpickler.__init__(self, self.file_handle)
        try:
            import numpy as np
        except ImportError:
            np = None
        self.np = np

    def load_build(self):
        
        Unpickler.load_build(self)

        
        if isinstance(self.stack[-1], (NDArrayWrapper, NumpyArrayWrapper)):
            if self.np is None:
                raise ImportError("Trying to unpickle an ndarray, "
                                  "but numpy didn't import correctly")
            array_wrapper = self.stack.pop()
            
            
            
            if isinstance(array_wrapper, NDArrayWrapper):
                self.compat_mode = True
            self.stack.append(array_wrapper.read(self))

    
    dispatch[pickle.BUILD[0]] = load_build





def dump(value, filename, compress=0, protocol=None, cache_size=None):
    

    if Path is not None and isinstance(filename, Path):
        filename = str(filename)

    is_filename = isinstance(filename, str)
    is_fileobj = hasattr(filename, "write")

    compress_method = 'zlib'  
    if compress is True:
        
        
        compress_level = None
    elif isinstance(compress, tuple):
        
        if len(compress) != 2:
            raise ValueError(
                'Compress argument tuple should contain exactly 2 elements: '
                '(compress method, compress level), you passed {}'
                .format(compress))
        compress_method, compress_level = compress
    elif isinstance(compress, str):
        compress_method = compress
        compress_level = None  
        compress = (compress_method, compress_level)
    else:
        compress_level = compress

    if compress_method == 'lz4' and lz4 is None:
        raise ValueError(LZ4_NOT_INSTALLED_ERROR)

    if (compress_level is not None and
            compress_level is not False and
            compress_level not in range(10)):
        
        raise ValueError(
            'Non valid compress level given: "{}". Possible values are '
            '{}.'.format(compress_level, list(range(10))))

    if compress_method not in _COMPRESSORS:
        
        raise ValueError(
            'Non valid compression method given: "{}". Possible values are '
            '{}.'.format(compress_method, _COMPRESSORS))

    if not is_filename and not is_fileobj:
        
        
        raise ValueError(
            'Second argument should be a filename or a file-like object, '
            '%s (type %s) was given.'
            % (filename, type(filename))
        )

    if is_filename and not isinstance(compress, tuple):
        
        
        

        
        compress_method = None
        for name, compressor in _COMPRESSORS.items():
            if filename.endswith(compressor.extension):
                compress_method = name

        if compress_method in _COMPRESSORS and compress_level == 0:
            
            
            compress_level = None

    if cache_size is not None:
        
        warnings.warn("Please do not set 'cache_size' in joblib.dump, "
                      "this parameter has no effect and will be removed. "
                      "You used 'cache_size={}'".format(cache_size),
                      DeprecationWarning, stacklevel=2)

    if compress_level != 0:
        with _write_fileobject(filename, compress=(compress_method,
                                                   compress_level)) as f:
            NumpyPickler(f, protocol=protocol).dump(value)
    elif is_filename:
        with open(filename, 'wb') as f:
            NumpyPickler(f, protocol=protocol).dump(value)
    else:
        NumpyPickler(filename, protocol=protocol).dump(value)

    
    if is_fileobj:
        return

    
    
    return [filename]


def _unpickle(fobj, filename="", mmap_mode=None):
    
    
    
    
    
    
    
    unpickler = NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode)
    obj = None
    try:
        obj = unpickler.load()
        if unpickler.compat_mode:
            warnings.warn("The file '%s' has been generated with a "
                          "joblib version less than 0.10. "
                          "Please regenerate this pickle file."
                          % filename,
                          DeprecationWarning, stacklevel=3)
    except UnicodeDecodeError as exc:
        
        new_exc = ValueError(
            'You may be trying to read with '
            'python 3 a joblib pickle generated with python 2. '
            'This feature is not supported by joblib.')
        new_exc.__cause__ = exc
        raise new_exc
    return obj


def load_temporary_memmap(filename, mmap_mode, unlink_on_gc_collect):
    from ._memmapping_reducer import JOBLIB_MMAPS, add_maybe_unlink_finalizer
    obj = load(filename, mmap_mode)
    JOBLIB_MMAPS.add(obj.filename)
    if unlink_on_gc_collect:
        add_maybe_unlink_finalizer(obj)
    return obj


def load(filename, mmap_mode=None):
    
    if Path is not None and isinstance(filename, Path):
        filename = str(filename)

    if hasattr(filename, "read"):
        fobj = filename
        filename = getattr(fobj, 'name', '')
        with _read_fileobject(fobj, filename, mmap_mode) as fobj:
            obj = _unpickle(fobj)
    else:
        with open(filename, 'rb') as f:
            with _read_fileobject(f, filename, mmap_mode) as fobj:
                if isinstance(fobj, str):
                    
                    
                    
                    return load_compatibility(fobj)

                obj = _unpickle(fobj, filename, mmap_mode)
    return obj
