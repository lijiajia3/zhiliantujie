




from math import log, sqrt
from numbers import Integral, Real

import numpy as np
from scipy import linalg
from scipy.sparse import issparse
from scipy.sparse.linalg import svds
from scipy.special import gammaln

from ..base import _fit_context
from ..utils import check_random_state
from ..utils._arpack import _init_arpack_v0
from ..utils._array_api import _convert_to_numpy, get_namespace
from ..utils._param_validation import Interval, RealNotInt, StrOptions
from ..utils.extmath import fast_logdet, randomized_svd, stable_cumsum, svd_flip
from ..utils.sparsefuncs import _implicit_column_offset, mean_variance_axis
from ..utils.validation import check_is_fitted, validate_data
from ._base import _BasePCA


def _assess_dimension(spectrum, rank, n_samples):
    
    xp, _ = get_namespace(spectrum)

    n_features = spectrum.shape[0]
    if not 1 <= rank < n_features:
        raise ValueError("the tested rank should be in [1, n_features - 1]")

    eps = 1e-15

    if spectrum[rank - 1] < eps:
        
        
        
        
        
        return -xp.inf

    pu = -rank * log(2.0)
    for i in range(1, rank + 1):
        pu += (
            gammaln((n_features - i + 1) / 2.0)
            - log(xp.pi) * (n_features - i + 1) / 2.0
        )

    pl = xp.sum(xp.log(spectrum[:rank]))
    pl = -pl * n_samples / 2.0

    v = max(eps, xp.sum(spectrum[rank:]) / (n_features - rank))
    pv = -log(v) * n_samples * (n_features - rank) / 2.0

    m = n_features * rank - rank * (rank + 1.0) / 2.0
    pp = log(2.0 * xp.pi) * (m + rank) / 2.0

    pa = 0.0
    spectrum_ = xp.asarray(spectrum, copy=True)
    spectrum_[rank:n_features] = v
    for i in range(rank):
        for j in range(i + 1, spectrum.shape[0]):
            pa += log(
                (spectrum[i] - spectrum[j]) * (1.0 / spectrum_[j] - 1.0 / spectrum_[i])
            ) + log(n_samples)

    ll = pu + pl + pv + pp - pa / 2.0 - rank * log(n_samples) / 2.0

    return ll


def _infer_dimension(spectrum, n_samples):
    
    xp, _ = get_namespace(spectrum)

    ll = xp.empty_like(spectrum)
    ll[0] = -xp.inf  
    for rank in range(1, spectrum.shape[0]):
        ll[rank] = _assess_dimension(spectrum, rank, n_samples)
    return xp.argmax(ll)


class PCA(_BasePCA):
    

    _parameter_constraints: dict = {
        "n_components": [
            Interval(Integral, 0, None, closed="left"),
            Interval(RealNotInt, 0, 1, closed="neither"),
            StrOptions({"mle"}),
            None,
        ],
        "copy": ["boolean"],
        "whiten": ["boolean"],
        "svd_solver": [
            StrOptions({"auto", "full", "covariance_eigh", "arpack", "randomized"})
        ],
        "tol": [Interval(Real, 0, None, closed="left")],
        "iterated_power": [
            StrOptions({"auto"}),
            Interval(Integral, 0, None, closed="left"),
        ],
        "n_oversamples": [Interval(Integral, 1, None, closed="left")],
        "power_iteration_normalizer": [StrOptions({"auto", "QR", "LU", "none"})],
        "random_state": ["random_state"],
    }

    def __init__(
        self,
        n_components=None,
        *,
        copy=True,
        whiten=False,
        svd_solver="auto",
        tol=0.0,
        iterated_power="auto",
        n_oversamples=10,
        power_iteration_normalizer="auto",
        random_state=None,
    ):
        self.n_components = n_components
        self.copy = copy
        self.whiten = whiten
        self.svd_solver = svd_solver
        self.tol = tol
        self.iterated_power = iterated_power
        self.n_oversamples = n_oversamples
        self.power_iteration_normalizer = power_iteration_normalizer
        self.random_state = random_state

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y=None):
        
        self._fit(X)
        return self

    @_fit_context(prefer_skip_nested_validation=True)
    def fit_transform(self, X, y=None):
        
        U, S, _, X, x_is_centered, xp = self._fit(X)
        if U is not None:
            U = U[:, : self.n_components_]

            if self.whiten:
                
                U *= sqrt(X.shape[0] - 1)
            else:
                
                U *= S[: self.n_components_]

            return U
        else:  
            return self._transform(X, xp, x_is_centered=x_is_centered)

    def _fit(self, X):
        
        xp, is_array_api_compliant = get_namespace(X)

        
        if issparse(X) and self.svd_solver not in ["auto", "arpack", "covariance_eigh"]:
            raise TypeError(
                'PCA only support sparse inputs with the "arpack" and'
                f' "covariance_eigh" solvers, while "{self.svd_solver}" was passed. See'
                " TruncatedSVD for a possible alternative."
            )
        if self.svd_solver == "arpack" and is_array_api_compliant:
            raise ValueError(
                "PCA with svd_solver='arpack' is not supported for Array API inputs."
            )

        
        
        
        
        
        
        X = validate_data(
            self,
            X,
            dtype=[xp.float64, xp.float32],
            force_writeable=True,
            accept_sparse=("csr", "csc"),
            ensure_2d=True,
            copy=False,
        )
        self._fit_svd_solver = self.svd_solver
        if self._fit_svd_solver == "auto" and issparse(X):
            self._fit_svd_solver = "arpack"

        if self.n_components is None:
            if self._fit_svd_solver != "arpack":
                n_components = min(X.shape)
            else:
                n_components = min(X.shape) - 1
        else:
            n_components = self.n_components

        if self._fit_svd_solver == "auto":
            
            
            if X.shape[1] <= 1_000 and X.shape[0] >= 10 * X.shape[1]:
                self._fit_svd_solver = "covariance_eigh"
            
            elif max(X.shape) <= 500 or n_components == "mle":
                self._fit_svd_solver = "full"
            elif 1 <= n_components < 0.8 * min(X.shape):
                self._fit_svd_solver = "randomized"
            
            else:
                self._fit_svd_solver = "full"

        
        if self._fit_svd_solver in ("full", "covariance_eigh"):
            return self._fit_full(X, n_components, xp, is_array_api_compliant)
        elif self._fit_svd_solver in ["arpack", "randomized"]:
            return self._fit_truncated(X, n_components, xp)

    def _fit_full(self, X, n_components, xp, is_array_api_compliant):
        
        n_samples, n_features = X.shape

        if n_components == "mle":
            if n_samples < n_features:
                raise ValueError(
                    "n_components='mle' is only supported if n_samples >= n_features"
                )
        elif not 0 <= n_components <= min(n_samples, n_features):
            raise ValueError(
                f"n_components={n_components} must be between 0 and "
                f"min(n_samples, n_features)={min(n_samples, n_features)} with "
                f"svd_solver={self._fit_svd_solver!r}"
            )

        self.mean_ = xp.mean(X, axis=0)
        
        
        
        
        
        self.mean_ = xp.reshape(xp.asarray(self.mean_), (-1,))

        if self._fit_svd_solver == "full":
            X_centered = xp.asarray(X, copy=True) if self.copy else X
            X_centered -= self.mean_
            x_is_centered = not self.copy

            if not is_array_api_compliant:
                
                
                
                
                
                
                
                U, S, Vt = linalg.svd(X_centered, full_matrices=False)
            else:
                U, S, Vt = xp.linalg.svd(X_centered, full_matrices=False)
            explained_variance_ = (S**2) / (n_samples - 1)

        else:
            assert self._fit_svd_solver == "covariance_eigh"
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            x_is_centered = False
            C = X.T @ X
            C -= (
                n_samples
                * xp.reshape(self.mean_, (-1, 1))
                * xp.reshape(self.mean_, (1, -1))
            )
            C /= n_samples - 1
            eigenvals, eigenvecs = xp.linalg.eigh(C)

            
            
            
            
            
            
            
            eigenvals = xp.reshape(xp.asarray(eigenvals), (-1,))
            eigenvecs = xp.asarray(eigenvecs)

            eigenvals = xp.flip(eigenvals, axis=0)
            eigenvecs = xp.flip(eigenvecs, axis=1)

            
            
            
            
            eigenvals[eigenvals < 0.0] = 0.0
            explained_variance_ = eigenvals

            
            
            S = xp.sqrt(eigenvals * (n_samples - 1))
            Vt = eigenvecs.T
            U = None

        
        U, Vt = svd_flip(U, Vt, u_based_decision=False)

        components_ = Vt

        
        total_var = xp.sum(explained_variance_)
        explained_variance_ratio_ = explained_variance_ / total_var
        singular_values_ = xp.asarray(S, copy=True)  

        
        if n_components == "mle":
            n_components = _infer_dimension(explained_variance_, n_samples)
        elif 0 < n_components < 1.0:
            
            
            
            
            
            if is_array_api_compliant:
                
                
                
                
                
                
                
                
                
                explained_variance_ratio_np = _convert_to_numpy(
                    explained_variance_ratio_, xp=xp
                )
            else:
                explained_variance_ratio_np = explained_variance_ratio_
            ratio_cumsum = stable_cumsum(explained_variance_ratio_np)
            n_components = np.searchsorted(ratio_cumsum, n_components, side="right") + 1

        
        
        if n_components < min(n_features, n_samples):
            self.noise_variance_ = xp.mean(explained_variance_[n_components:])
        else:
            self.noise_variance_ = 0.0

        self.n_samples_ = n_samples
        self.n_components_ = n_components
        
        
        
        
        
        
        self.components_ = xp.asarray(components_[:n_components, :], copy=True)

        
        self.explained_variance_ = xp.asarray(
            explained_variance_[:n_components], copy=True
        )
        self.explained_variance_ratio_ = xp.asarray(
            explained_variance_ratio_[:n_components], copy=True
        )
        self.singular_values_ = xp.asarray(singular_values_[:n_components], copy=True)

        return U, S, Vt, X, x_is_centered, xp

    def _fit_truncated(self, X, n_components, xp):
        
        n_samples, n_features = X.shape

        svd_solver = self._fit_svd_solver
        if isinstance(n_components, str):
            raise ValueError(
                "n_components=%r cannot be a string with svd_solver='%s'"
                % (n_components, svd_solver)
            )
        elif not 1 <= n_components <= min(n_samples, n_features):
            raise ValueError(
                "n_components=%r must be between 1 and "
                "min(n_samples, n_features)=%r with "
                "svd_solver='%s'"
                % (n_components, min(n_samples, n_features), svd_solver)
            )
        elif svd_solver == "arpack" and n_components == min(n_samples, n_features):
            raise ValueError(
                "n_components=%r must be strictly less than "
                "min(n_samples, n_features)=%r with "
                "svd_solver='%s'"
                % (n_components, min(n_samples, n_features), svd_solver)
            )

        random_state = check_random_state(self.random_state)

        
        total_var = None
        if issparse(X):
            self.mean_, var = mean_variance_axis(X, axis=0)
            total_var = var.sum() * n_samples / (n_samples - 1)  
            X_centered = _implicit_column_offset(X, self.mean_)
            x_is_centered = False
        else:
            self.mean_ = xp.mean(X, axis=0)
            X_centered = xp.asarray(X, copy=True) if self.copy else X
            X_centered -= self.mean_
            x_is_centered = not self.copy

        if svd_solver == "arpack":
            v0 = _init_arpack_v0(min(X.shape), random_state)
            U, S, Vt = svds(X_centered, k=n_components, tol=self.tol, v0=v0)
            
            
            S = S[::-1]
            
            U, Vt = svd_flip(U[:, ::-1], Vt[::-1], u_based_decision=False)

        elif svd_solver == "randomized":
            
            U, S, Vt = randomized_svd(
                X_centered,
                n_components=n_components,
                n_oversamples=self.n_oversamples,
                n_iter=self.iterated_power,
                power_iteration_normalizer=self.power_iteration_normalizer,
                flip_sign=False,
                random_state=random_state,
            )
            U, Vt = svd_flip(U, Vt, u_based_decision=False)

        self.n_samples_ = n_samples
        self.components_ = Vt
        self.n_components_ = n_components

        
        self.explained_variance_ = (S**2) / (n_samples - 1)

        
        
        
        
        
        
        
        if total_var is None:
            N = X.shape[0] - 1
            X_centered **= 2
            total_var = xp.sum(X_centered) / N

        self.explained_variance_ratio_ = self.explained_variance_ / total_var
        self.singular_values_ = xp.asarray(S, copy=True)  

        if self.n_components_ < min(n_features, n_samples):
            self.noise_variance_ = total_var - xp.sum(self.explained_variance_)
            self.noise_variance_ /= min(n_features, n_samples) - n_components
        else:
            self.noise_variance_ = 0.0

        return U, S, Vt, X, x_is_centered, xp

    def score_samples(self, X):
        
        check_is_fitted(self)
        xp, _ = get_namespace(X)
        X = validate_data(self, X, dtype=[xp.float64, xp.float32], reset=False)
        Xr = X - self.mean_
        n_features = X.shape[1]
        precision = self.get_precision()
        log_like = -0.5 * xp.sum(Xr * (Xr @ precision), axis=1)
        log_like -= 0.5 * (n_features * log(2.0 * np.pi) - fast_logdet(precision))
        return log_like

    def score(self, X, y=None):
        
        xp, _ = get_namespace(X)
        return float(xp.mean(self.score_samples(X)))

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.transformer_tags.preserves_dtype = ["float64", "float32"]
        tags.array_api_support = True
        tags.input_tags.sparse = self.svd_solver in (
            "auto",
            "arpack",
            "covariance_eigh",
        )
        return tags
