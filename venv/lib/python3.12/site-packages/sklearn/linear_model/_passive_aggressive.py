


from numbers import Real

from ..base import _fit_context
from ..utils._param_validation import Interval, StrOptions
from ._stochastic_gradient import DEFAULT_EPSILON, BaseSGDClassifier, BaseSGDRegressor


class PassiveAggressiveClassifier(BaseSGDClassifier):
    

    _parameter_constraints: dict = {
        **BaseSGDClassifier._parameter_constraints,
        "loss": [StrOptions({"hinge", "squared_hinge"})],
        "C": [Interval(Real, 0, None, closed="right")],
    }

    def __init__(
        self,
        *,
        C=1.0,
        fit_intercept=True,
        max_iter=1000,
        tol=1e-3,
        early_stopping=False,
        validation_fraction=0.1,
        n_iter_no_change=5,
        shuffle=True,
        verbose=0,
        loss="hinge",
        n_jobs=None,
        random_state=None,
        warm_start=False,
        class_weight=None,
        average=False,
    ):
        super().__init__(
            penalty=None,
            fit_intercept=fit_intercept,
            max_iter=max_iter,
            tol=tol,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            shuffle=shuffle,
            verbose=verbose,
            random_state=random_state,
            eta0=1.0,
            warm_start=warm_start,
            class_weight=class_weight,
            average=average,
            n_jobs=n_jobs,
        )

        self.C = C
        self.loss = loss

    @_fit_context(prefer_skip_nested_validation=True)
    def partial_fit(self, X, y, classes=None):
        
        if not hasattr(self, "classes_"):
            self._more_validate_params(for_partial_fit=True)

            if self.class_weight == "balanced":
                raise ValueError(
                    "class_weight 'balanced' is not supported for "
                    "partial_fit. For 'balanced' weights, use "
                    "`sklearn.utils.compute_class_weight` with "
                    "`class_weight='balanced'`. In place of y you "
                    "can use a large enough subset of the full "
                    "training set target to properly estimate the "
                    "class frequency distributions. Pass the "
                    "resulting weights as the class_weight "
                    "parameter."
                )

        lr = "pa1" if self.loss == "hinge" else "pa2"
        return self._partial_fit(
            X,
            y,
            alpha=1.0,
            C=self.C,
            loss="hinge",
            learning_rate=lr,
            max_iter=1,
            classes=classes,
            sample_weight=None,
            coef_init=None,
            intercept_init=None,
        )

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, coef_init=None, intercept_init=None):
        
        self._more_validate_params()

        lr = "pa1" if self.loss == "hinge" else "pa2"
        return self._fit(
            X,
            y,
            alpha=1.0,
            C=self.C,
            loss="hinge",
            learning_rate=lr,
            coef_init=coef_init,
            intercept_init=intercept_init,
        )


class PassiveAggressiveRegressor(BaseSGDRegressor):
    

    _parameter_constraints: dict = {
        **BaseSGDRegressor._parameter_constraints,
        "loss": [StrOptions({"epsilon_insensitive", "squared_epsilon_insensitive"})],
        "C": [Interval(Real, 0, None, closed="right")],
        "epsilon": [Interval(Real, 0, None, closed="left")],
    }

    def __init__(
        self,
        *,
        C=1.0,
        fit_intercept=True,
        max_iter=1000,
        tol=1e-3,
        early_stopping=False,
        validation_fraction=0.1,
        n_iter_no_change=5,
        shuffle=True,
        verbose=0,
        loss="epsilon_insensitive",
        epsilon=DEFAULT_EPSILON,
        random_state=None,
        warm_start=False,
        average=False,
    ):
        super().__init__(
            penalty=None,
            l1_ratio=0,
            epsilon=epsilon,
            eta0=1.0,
            fit_intercept=fit_intercept,
            max_iter=max_iter,
            tol=tol,
            early_stopping=early_stopping,
            validation_fraction=validation_fraction,
            n_iter_no_change=n_iter_no_change,
            shuffle=shuffle,
            verbose=verbose,
            random_state=random_state,
            warm_start=warm_start,
            average=average,
        )
        self.C = C
        self.loss = loss

    @_fit_context(prefer_skip_nested_validation=True)
    def partial_fit(self, X, y):
        
        if not hasattr(self, "coef_"):
            self._more_validate_params(for_partial_fit=True)

        lr = "pa1" if self.loss == "epsilon_insensitive" else "pa2"
        return self._partial_fit(
            X,
            y,
            alpha=1.0,
            C=self.C,
            loss="epsilon_insensitive",
            learning_rate=lr,
            max_iter=1,
            sample_weight=None,
            coef_init=None,
            intercept_init=None,
        )

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, coef_init=None, intercept_init=None):
        
        self._more_validate_params()

        lr = "pa1" if self.loss == "epsilon_insensitive" else "pa2"
        return self._fit(
            X,
            y,
            alpha=1.0,
            C=self.C,
            loss="epsilon_insensitive",
            learning_rate=lr,
            coef_init=coef_init,
            intercept_init=intercept_init,
        )
